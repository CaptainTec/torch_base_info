{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@[TOC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.关于Variable和Tensor\n",
    "\n",
    "旧版本的Pytorch中，Variable是对Tensor的一个封装；在Pytorch大于v0.4的版本后，Varible和Tensor合并了，意味着Tensor可以像旧版本的Variable那样运行，当然新版本中Variable封装仍旧可以用，但是对Varieble操作返回的将是一个Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(3, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad  # 默认为False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = Variable(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 torch.Tensor和torch.tensor有区别\n",
    "\n",
    "#### (1) 类\n",
    "在PyTorch 中，`torch.Tensor`是主要的tensor类，所有的tensor都是`torch.Tensor`的实例。\n",
    "`torch.Tensor`是`torch.FloatTensor`的别名。\n",
    "\n",
    "而torch.tensor是一个函数，返回的是一个tensor，在PyTorch官方文档中，描述如下：\n",
    "```python\n",
    "torch.tensor(data, dtype=None, device=None, requires_grad=False) → Tensor\n",
    "Constructs a tensor with data.\n",
    "```\n",
    "\n",
    "#### (2) dtype类型\n",
    "所以需要注意的一点是：\n",
    "\n",
    "- torch.Tensor（data）是将输入的data转化`torch.FloatTensor`\n",
    "- torch.tensor(data):(当你未指定`dtype`的类型时)将data转化为`torch.FloatTensor`、`torch.LongTensor`、`torch.DoubleTensor`等类型，转化类型依据于data的类型或者`dtype`的值\n",
    "\n",
    "\n",
    "#### (3)创建空tensor\n",
    "使用如下语句：tensor_without_data = torch.Tensor()可以创建一个空的FloatTensor，而当你使用tensor_without_data = torch.tensor()时候则会报错\n",
    "\n",
    "当你想要创建一个空的tensor时候，可以使用如下的方法：\n",
    "\n",
    "- tensor_without_data = torch.Tensor() # tensor([ ])\n",
    "- tensor_without_data = torch.tensor(( )) # tensor([ ])\n",
    "- tensor_without_data = torch.empty([ ]) # tensor(0.)\n",
    "\n",
    "所以`torch.Tensor`应该说是同时具有`torch.tensor`和`torch.empty`的功能，但是使用`torch.Tensor`可能会使你的代码confusing，所以最好还是使用`torch.tensor`和`torch.empty`，而不是`torch.Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "False\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: volatile was removed (Variable.volatile is always False)\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed (Variable.volatile is always False)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    " \n",
    "a = t.ones(3,requires_grad=True)\n",
    "print(type(a))\n",
    "#输出：<class 'torch.Tensor'>\n",
    " \n",
    "a=Variable(a)\n",
    "print(type(a))\n",
    "#输出仍旧是：<class 'torch.Tensor'>\n",
    " \n",
    "print(a.volatile)\n",
    "#输出：__main__:1: UserWarning: volatile was removed (Variable.volatile is always False)\n",
    "a.volatile=True\n",
    "print(a.volatile)\n",
    "#输出：__main__:1: UserWarning: volatile was removed (Variable.volatile is always False)\n",
    "#现版本pytorch中移除了volatile这个属性，即volatile总是false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.叶子节点leaf\n",
    "\n",
    "对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的grad_fn为None。\n",
    "\n",
    "**grad_fn 指向Function对象，用于反向传播的梯度计算之用**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.], requires_grad=True) True\n",
      "tensor([0.1666, 0.6333, 0.2296], requires_grad=True)\n",
      "False\n",
      "None\n",
      "<MulBackward0 object at 0x125517be0>\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "a = t.ones(3,requires_grad=True)\n",
    "b = t.rand(3,requires_grad=True)\n",
    "print(a, a.is_leaf)  # 输出：(tensor([1., 1., 1.], requires_grad=True), True)\n",
    "\n",
    "print(b)  # 输出：tensor([0.4254, 0.8763, 0.5901], requires_grad=True)\n",
    "\n",
    " \n",
    "c = a*b\n",
    "print(c.is_leaf)  # 输出：False.说明c不是叶子节点\n",
    "\n",
    "print(a.grad_fn)  # 输出：None.叶子节点的grad_fn为None.\n",
    "\n",
    "print(c.grad_fn)  # 输出：<MulBackward0 object at 0x7fa45c406278> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. a为Tensor变量. 有很多方法和属性 可以用help(a)查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method backward in module torch.tensor:\n",
      "\n",
      "backward(gradient=None, retain_graph=None, create_graph=False) method of torch.Tensor instance\n",
      "    Computes the gradient of current tensor w.r.t. graph leaves.\n",
      "    \n",
      "    The graph is differentiated using the chain rule. If the tensor is\n",
      "    non-scalar (i.e. its data has more than one element) and requires\n",
      "    gradient, the function additionally requires specifying ``gradient``.\n",
      "    It should be a tensor of matching type and location, that contains\n",
      "    the gradient of the differentiated function w.r.t. ``self``.\n",
      "    \n",
      "    This function accumulates gradients in the leaves - you might need to\n",
      "    zero them before calling it.\n",
      "    \n",
      "    Arguments:\n",
      "        gradient (Tensor or None): Gradient w.r.t. the\n",
      "            tensor. If it is a tensor, it will be automatically converted\n",
      "            to a Tensor that does not require grad unless ``create_graph`` is True.\n",
      "            None values can be specified for scalar Tensors or ones that\n",
      "            don't require grad. If a None value would be acceptable then\n",
      "            this argument is optional.\n",
      "        retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      "            the grads will be freed. Note that in nearly all cases setting\n",
      "            this option to True is not needed and often can be worked around\n",
      "            in a much more efficient way. Defaults to the value of\n",
      "            ``create_graph``.\n",
      "        create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      "            be constructed, allowing to compute higher order derivative\n",
      "            products. Defaults to ``False``.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(a.backward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function expand:\n",
      "\n",
      "expand(...) method of torch.Tensor instance\n",
      "    expand(*sizes) -> Tensor\n",
      "    \n",
      "    Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n",
      "    to a larger size.\n",
      "    \n",
      "    Passing -1 as the size for a dimension means not changing the size of\n",
      "    that dimension.\n",
      "    \n",
      "    Tensor can be also expanded to a larger number of dimensions, and the\n",
      "    new ones will be appended at the front. For the new dimensions, the\n",
      "    size cannot be set to -1.\n",
      "    \n",
      "    Expanding a tensor does not allocate new memory, but only creates a\n",
      "    new view on the existing tensor where a dimension of size one is\n",
      "    expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
      "    of size 1 can be expanded to an arbitrary value without allocating new\n",
      "    memory.\n",
      "    \n",
      "    Args:\n",
      "        *sizes (torch.Size or int...): the desired expanded size\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "        More than one element of an expanded tensor may refer to a single\n",
      "        memory location. As a result, in-place operations (especially ones that\n",
      "        are vectorized) may result in incorrect behavior. If you need to write\n",
      "        to the tensors, please clone them first.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.tensor([[1], [2], [3]])\n",
      "        >>> x.size()\n",
      "        torch.Size([3, 1])\n",
      "        >>> x.expand(3, 4)\n",
      "        tensor([[ 1,  1,  1,  1],\n",
      "                [ 2,  2,  2,  2],\n",
      "                [ 3,  3,  3,  3]])\n",
      "        >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
      "        tensor([[ 1,  1,  1,  1],\n",
      "                [ 2,  2,  2,  2],\n",
      "                [ 3,  3,  3,  3]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(a.expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on NoneType object:\n",
      "\n",
      "class NoneType(object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      self != 0\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1], [2], [3]])\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.expand(3, 4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2],\n",
       "        [3, 3, 3, 3]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.expand(-1, 4)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y == z  # 两者相等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 求导 - autograd操作\n",
    "\n",
    "首先Tensor是默认不需要求导的，即requires_grad默认为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.ones(3)\n",
    "d.requires_grad\n",
    "#输出：False.Tensor默认不需要求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(3)\n",
    "b = torch.ones(3,requires_grad=True)\n",
    "print(b.requires_grad)  # 输出：True, 注意 a.requires_grad 仍然为False\n",
    "\n",
    "c = a + b\n",
    "print(c.requires_grad)  # 输出：True.虽然c没有指定需要求导，然是c依赖于b，而b需要求导，所以c.requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**并不是只有scaler才能进行backward操作，矩阵和向量也可以，只不过backward()中要添加对应维度的参数。**\n",
    "\n",
    "- retain_graph=True是为了保存中间缓存，否则再次backward的时候会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: tensor([1., 1., 1.], requires_grad=True) tensor([0.6488, 0.8633, 0.6592], requires_grad=True)\n",
      "2: tensor([0.6488, 0.8633, 0.6592], grad_fn=<MulBackward0>)\n",
      "4: None\n",
      "5: tensor([0.6488, 0.8633, 0.6592])\n",
      "6: tensor([1., 1., 1.])\n",
      "7: tensor([2., 2., 2.])\n",
      "8: tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "a = torch.ones(3,requires_grad=True)\n",
    "b = torch.rand(3,requires_grad=True)\n",
    "\n",
    "# 输出：(tensor([1., 1., 1.], requires_grad=True), \n",
    "# tensor([0.9373, 0.0556, 0.6426], requires_grad=True))\n",
    "print('1:', a, b)\n",
    "\n",
    "c = a*b\n",
    "print('2:', c)  # 输出：tensor([0.9373, 0.0556, 0.6426], grad_fn=<MulBackward0>)\n",
    "\n",
    "\n",
    "# 报错：RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "# 只有数值scalar才能进行backward操作\n",
    "# print('3:', c.backward(retain_graph=True))  # 报错\n",
    "\n",
    "d = c.sum()\n",
    "print('4:', d.backward(retain_graph=True))\n",
    "# retain_graph=True是为了保存中间缓存，否则再次backward的时候会报错\n",
    "\n",
    "print('5:', a.grad)  # a的导数是b\n",
    "#输出：tensor([0.9373, 0.0556, 0.6426])\n",
    "\n",
    "print('6:', b.grad)  # b的导数是a\n",
    "#输出：tensor([1., 1., 1.])\n",
    "#backward后a和b的grad产生了数值\n",
    "\n",
    "e = c.sum()\n",
    "e.backward(retain_graph=True)\n",
    "print('7:', b.grad)\n",
    "#输出：tensor([2., 2., 2.]).b的grad进行了两次backward后进行了累加.\n",
    "\n",
    "f = c.sum()\n",
    "print('8:', b.grad)\n",
    "#输出：tensor([2., 2., 2.])\n",
    "#只进行计算不backward，梯度不更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tensor.data和Tensor.detach()\n",
    "\n",
    "如过tensor的数值需要参与计算又不想参与到计算图的更新中，计算的时候可以用tensor.data，这样既能利用tensor的数值，又不会更新梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n",
      "<bound method Tensor.backward of tensor(4.9418)>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "a = torch.ones(3,4,requires_grad=True)\n",
    "b = torch.rand(3,4,requires_grad=True)\n",
    " \n",
    "print(a.data.requires_grad, a.requires_grad)  # False True # 因为a.data独立于计算图之外\n",
    "\n",
    "c = a.data * b.data\n",
    "d = c.sum()\n",
    "print(d.backward)\n",
    "# print(d.backward())  # 报错\n",
    "# 输出：RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "# 因为独立于计算图之外，requires_grad = False所以不能backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.backward of tensor(4.9418, grad_fn=<SumBackward0>)>\n"
     ]
    }
   ],
   "source": [
    "c = a * b\n",
    "d = c.sum()\n",
    "print(d.backward)  # 注意这个d中有grad_fn属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当tensor.data被修改的时候，tensor也会同步的被修改，此时用该tensor进行计算并backward的时候梯度的值就不再准确了，因为tensor已经被修改了！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311]])\n",
      "2: tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "a = torch.ones(3,4,requires_grad=True)\n",
    "b = torch.rand(3,4,requires_grad=True)\n",
    "c = a*b\n",
    "d = c.sum()\n",
    "print('1:', a.data.sigmoid_())\n",
    "#输出：tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
    "#        [0.7311, 0.7311, 0.7311, 0.7311],\n",
    "#        [0.7311, 0.7311, 0.7311, 0.7311]])\n",
    "#虽然对a.data进行sigmoid操作，但是a的值已经被修改了.\n",
    "\n",
    "d.backward()\n",
    "print('2:', b.grad)\n",
    "#输出：tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
    "#        [0.7311, 0.7311, 0.7311, 0.7311],\n",
    "#        [0.7311, 0.7311, 0.7311, 0.7311]])\n",
    "#b的grad不准了，本来应该都是1！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**为了避免因为对tensor.data修改导致grad变化的情况，可以利用tensor.detach，同样可以保证tensor不参与到计算图当中，\n",
    "但是当tensor的值被改变的时候，再进行backward就会 __报错__ 而不会有先前的因为tensor的值被改变而导致不准的情况了。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)\n",
      "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
      "        [0.7311, 0.7311, 0.7311, 0.7311]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "a = torch.ones(3,4,requires_grad=True)\n",
    "b = torch.rand(3,4,requires_grad=True)\n",
    "c = a * b\n",
    "d = c.sum()\n",
    "a_ = a.detach()\n",
    "a_.sigmoid_()\n",
    "print(a)\n",
    "print(a_)\n",
    "#输出：tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
    "#        [0.7311, 0.7311, 0.7311, 0.7311],\n",
    "#        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)\n",
    "#a的值已经发生了改变\n",
    "# d.backward()  # 报错\n",
    "#报错：RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n",
    "#因为a的值被修改了，所以不能再进行backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**推荐用tensor.detach的方式而不是tensor.data的方式，因为这样更保险！**\n",
    "\n",
    "# 6.autograd.grad和hook\n",
    "在计算的时候有时候我们可能会用到非叶节点的grad，但是非叶节点的grad在backward之后就会被自动清空："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9379, 0.0388, 0.9329, 0.9576],\n",
      "        [0.7599, 0.5928, 0.9593, 0.8715],\n",
      "        [0.4793, 0.1550, 0.8241, 0.1743]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "a = torch.ones(3,4,requires_grad=True)\n",
    "b = torch.rand(3,4,requires_grad=True)\n",
    "c = a*b\n",
    "d = c.sum()\n",
    "d.backward()\n",
    "print(a.grad)\n",
    "#输出：tensor([[0.3114, 0.3017, 0.8461, 0.6899],\n",
    "#        [0.3878, 0.8712, 0.2406, 0.7396],\n",
    "#        [0.6369, 0.0907, 0.4984, 0.5058]])\n",
    "\n",
    "print(c.grad)\n",
    "#输出：None\n",
    "#c为非叶子节点，计算后被清空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以用autograd.grad和hook来处理这种情况：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用autograd.grad获取中间节点梯度\n",
    "torch.autograd.grad(d,c)\n",
    "# 输出：(tensor([[1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.]]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用hook获取中间节点梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    " \n",
    "a = torch.ones(3,4,requires_grad=True)\n",
    "b = torch.rand(3,4,requires_grad=True)\n",
    "c = a*b\n",
    "d = c.sum()\n",
    " \n",
    "def print_grad(grad):\n",
    "    print(grad)\n",
    " \n",
    "# 给c注册hook\n",
    "c_hook = c.register_hook(print_grad)\n",
    " \n",
    "print(d.backward())\n",
    "#输出：tensor([[1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1.]])\n",
    " \n",
    "#移除钩子\n",
    "c_hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torch.nn in torch:\n",
      "\n",
      "NAME\n",
      "    torch.nn\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _VF\n",
      "    _functions (package)\n",
      "    _qscheme\n",
      "    _reduction\n",
      "    backends (package)\n",
      "    cpp\n",
      "    functional\n",
      "    grad\n",
      "    init\n",
      "    intrinsic (package)\n",
      "    modules (package)\n",
      "    parallel (package)\n",
      "    parameter\n",
      "    qat (package)\n",
      "    quantized (package)\n",
      "    utils (package)\n",
      "\n",
      "FILE\n",
      "    /anaconda3/lib/python3.7/site-packages/torch/nn/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "help(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sample = torch.ones(2, 2, requires_grad=True)\n",
    "target = torch.Tensor([[0, 1], [2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample), type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.dtype, target.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 nn.L1Loss\n",
    "\n",
    "$$\n",
    "loss(x, y) = \\frac{1}{N}\\sum_{i=1}^{N}|x-y|\n",
    "$$\n",
    "\n",
    "取预测值和真实值绝对误差的平均数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss()\n",
    "loss = criterion(sample, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 nn.SmoothL1Loss\n",
    "\n",
    "SmoothL1Loss也叫Huber Loss，误差在(-1, 1)上是平方损失，其他情况是L1损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6250, grad_fn=<SmoothL1LossBackward>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "loss = criterion(sample, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 nn.MSELoss\n",
    "\n",
    "$$\n",
    "loss(x, y) = \\frac{1}{N}\\sum_{i=1}^{N}(x-y)^2\n",
    "$$\n",
    "\n",
    "平方损失函数，计算公式是预测值和真实值的差值的平方再取平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(sample, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 nn.BCELoss\n",
    "\n",
    "$$\n",
    "loss(o, t) = \\frac{1}{N}\\sum_{i=1}^{N}[t_i*log(o_i)+(1-t_i)*log(1-o_i)]\n",
    "$$\n",
    "\n",
    "二分类交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-13.8155, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "loss = criterion(sample, target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 nn.CrossEntropyLoss\n",
    "\n",
    "交叉熵损失函数\n",
    "\n",
    "$$\n",
    "loss(x, label) = -log\\frac{e^{X_{label}}}{\\sum_{j=1}^{N}e^{X_j}} = -x_{label} + log{\\sum_{j=1}^{N}e^{X_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "该损失函数结合了`nn.LogSoftmax()`和`nn.NLLLoss()`两个函数。它在做分类（具体几类）训练的时候是非常有用的。在训练过程中，对于每个类分配权值，可选的参数权值应该是一个1D张量。当你有一个不平衡的训练集时，这是是非常有用的。那么针对这个函数，下面将做详细的介绍。\n",
    "\n",
    "### 7.5.1 什么是交叉熵？\n",
    "交叉熵主要是用来判定实际的输出与期望的输出的接近程度，为什么这么说呢，举个例子：在做分类的训练的时候，如果一个样本属于第K类，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即`[0,0,1,0,….0,0]`，这个数组也就是样本的Label，是神经网络最期望的输出结果。也就是说用它来衡量网络的输出与标签的差异，利用这种差异经过反向传播去更新网络参数。\n",
    "\n",
    "### 7.5.2 交叉熵原理？\n",
    "在说交叉熵之前，先说一下信息量与熵。\n",
    "\n",
    "**信息量** ：它是用来衡量一个事件的不确定性的；一个事件发生的概率越大，不确定性越小，则它所携带的信息量就越小。假设X是一个离散型随机变量，其取值集合为X，概率分布函数为$p(x) = P(X=x), x\\in X$ ，我们定义事件$X = x_0$ 的信息量为：$I(x_0) = -log(p(x_0))$ 当 $p(x_0)=1$ 时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。\n",
    "\n",
    "**熵**：它是用来衡量一个系统的混乱程度的，代表一个系统中信息量的总和；信息量总和越大，表明这个系统不确定性就越大。\n",
    "计算公式为：\n",
    "$$\n",
    "H(x) = -\\sum_{x_i}p(x_i)log(p(x_i))\n",
    "$$\n",
    "\n",
    "**交叉熵**：它主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出，$H(p, q)$ 为交叉熵，则\n",
    "$$\n",
    "H(p, q) = -\\sum_{x}(p(x)log(q(x))+(1-p(x))log(1-q(x)))\n",
    "$$\n",
    "\n",
    "### 7.5.3 Pytorch中的CrossEntropyLoss()函数\n",
    "实际上PyTorch中的CrossEntropyLoss的计算并不是采用的上式，而是：\n",
    "$$\n",
    "H(p, q) = -\\sum_{x}p(x)log(q(x))\n",
    "$$\n",
    "它是交叉熵的另外一种形式。\n",
    "\n",
    "Pytorch中CrossEntropyLoss()函数的主要是将softmax-log-NLLLoss合并到一块得到的结果。\n",
    "\n",
    "    1、Softmax后的数值都在0~1之间，所以ln之后值域是负无穷到0。\n",
    "\n",
    "    2、然后将Softmax之后的结果取log，将乘法改成加法减少计算量，同时保障函数的单调性 。\n",
    "\n",
    "    3、NLLLoss的结果就是把上面的输出与Label对应的那个值拿出来(下面例子中就是：将log_output\\logsoftmax_output中与y_target对应的值拿出来)，去掉负号，再求均值。 下面是我仿真写的一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input:\n",
      " tensor([[-0.1783,  1.3814,  0.4364],\n",
      "        [ 0.8832,  0.4122, -0.8587],\n",
      "        [-0.2509,  0.6555, -0.2324]])\n",
      "soft_output:\n",
      " tensor([[0.1315, 0.6254, 0.2431],\n",
      "        [0.5557, 0.3470, 0.0974],\n",
      "        [0.2225, 0.5508, 0.2267]])\n",
      "log_output:\n",
      " tensor([[-2.0290, -0.4693, -1.4143],\n",
      "        [-0.5875, -1.0586, -2.3294],\n",
      "        [-1.5028, -0.5964, -1.4843]])\n",
      "logsoftmax_output:\n",
      " tensor([[-2.0290, -0.4693, -1.4143],\n",
      "        [-0.5875, -1.0586, -2.3294],\n",
      "        [-1.5028, -0.5964, -1.4843]])\n",
      "nlloss_output:\n",
      " tensor(1.4338)\n",
      "crossentropyloss_output:\n",
      " tensor(1.4338)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x_input=torch.randn(3,3)#随机生成输入 \n",
    "print('x_input:\\n',x_input) \n",
    "y_target=torch.tensor([1,2,0])#设置输出具体值 print('y_target\\n',y_target)\n",
    "\n",
    "#计算输入softmax，此时可以看到每一行加到一起结果都是1\n",
    "softmax_func=nn.Softmax(dim=1)\n",
    "soft_output=softmax_func(x_input)\n",
    "print('soft_output:\\n',soft_output)\n",
    "\n",
    "#在softmax的基础上取log\n",
    "log_output=torch.log(soft_output)\n",
    "print('log_output:\\n',log_output)\n",
    "\n",
    "#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。\n",
    "logsoftmax_func=nn.LogSoftmax(dim=1)\n",
    "logsoftmax_output=logsoftmax_func(x_input)\n",
    "print('logsoftmax_output:\\n',logsoftmax_output)\n",
    "\n",
    "#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True\n",
    "nllloss_func=nn.NLLLoss()\n",
    "nlloss_output=nllloss_func(logsoftmax_output,y_target)\n",
    "print('nlloss_output:\\n',nlloss_output)\n",
    "\n",
    "#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样\n",
    "crossentropyloss=nn.CrossEntropyLoss()\n",
    "crossentropyloss_output=crossentropyloss(x_input,y_target)\n",
    "print('crossentropyloss_output:\\n',crossentropyloss_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 nn.NLLLoss\n",
    "负对数似然(损失)函数 - Negative Log Likelihood Loss\n",
    "$$\n",
    "loss(x, label) = -x_{label}\n",
    "$$\n",
    "\n",
    "`nn.NLLLoss` 和 `nn.CrossEntropyLoss`的功能是非常相似的！通常用在多分类模型中。\n",
    "\n",
    "- 示例见上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 nn.NLLLoss2D\n",
    "\n",
    "和上面类似，但是多了几个维度，一般用在图片上。\n",
    "- input (N, C, H, W)\n",
    "- target (N, H, W)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
