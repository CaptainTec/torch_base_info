{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**尝试了 tfidf + 各种机器学习模型，分数一直在0.93+ 左右**\n",
    "\n",
    "**尝试了 BiRNN 和 TextCNN，其中TextCNN 分数在0.94～0.95徘徊，可能是没有仔细调参的原因**\n",
    "\n",
    "**最后梭哈的单模型 BiRNN，采用不同学习率，最后一分钟提交的最高分数0.95686，排行榜第9，侥幸杀进Top10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Score Code - BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def read_comments(fname):\n",
    "    data = []\n",
    "    with open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            data.append([text, int(label)])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "total_data = read_comments(\"./data/Comments/train_shuffle.txt\")\n",
    "train_data = total_data[:16000]  # 调参完毕改成全部的数据，调参时用all_data[:14000]，划分训练集和验证集\n",
    "valid_data = total_data[14000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 19  # 最大长度\n",
    "# 预处理数据\n",
    "def get_tokenized_comments(data):\n",
    "    def tokenizer(text):\n",
    "        return list(text)\n",
    "    return [tokenizer(comment) for comment, _ in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_comments(data):\n",
    "    tokenized_data = get_tokenized_comments(data)\n",
    "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter, min_freq=2)\n",
    "\n",
    "\n",
    "def preprocess_comments(data, vocab, train_data=True):\n",
    "    def pad(x): \n",
    "        # 0 为 <unk> 即未登录次，1为<pad>, 即填充。但是wiki预训练的词向量中 <unk>和<pad>均是全0\n",
    "        # 故这里填0 或 1 均可，严格来说应该填 1\n",
    "        return x[:max_len] if len(x) > max_len else x + [1] * (max_len - len(x))\n",
    "    if train_data:\n",
    "        tokenized_data = get_tokenized_comments(data)\n",
    "        features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "        labels = torch.tensor([score for _, score in data])\n",
    "    else:\n",
    "        tokenized_data = [list(line.strip()) for line in data]\n",
    "        features = torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "        labels = None\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "X torch.Size([32, 19]) y torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches:', 500)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数据集\n",
    "vocab = get_vocab_comments(train_data)\n",
    "print(len(vocab))\n",
    "batch_size = 32\n",
    "train_set = Data.TensorDataset(*preprocess_comments(train_data, vocab))\n",
    "valid_set = Data.TensorDataset(*preprocess_comments(valid_data, vocab))\n",
    "train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "valid_iter = Data.DataLoader(valid_set, batch_size)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    break\n",
    "'#batches:', len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class BiRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.encoder = nn.LSTM(input_size=embed_size,\n",
    "                               hidden_size=num_hiddens,\n",
    "                               num_layers=num_layers,\n",
    "                               bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        dropout = self.dropout(encoding)\n",
    "        outs = self.decoder(dropout)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 300, 50, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)  # 构建model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim 加载预训练的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 用load_word2vec_format快速打开词向量\n",
    "w2v = KeyedVectors.load_word2vec_format('./data/Comments/sgns.wiki.word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    '''\n",
    "    @params:\n",
    "        words: 需要加载词向量的词语列表，以 itos (index to string) 的词典形式给出\n",
    "        pretrained_vocab: 预训练词向量\n",
    "    @return:\n",
    "        embed: 加载到的词向量\n",
    "    '''\n",
    "    embed = torch.zeros(len(words), pretrained_vocab.vectors[0].shape[0]) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in pretrained_vocab:\n",
    "            embed[i,:] = torch.Tensor([0]*pretrained_vocab.vectors[0].shape[0])\n",
    "        else:\n",
    "            embed[i,:] = torch.Tensor(pretrained_vocab[word])\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    else:\n",
    "        print(\"Yeah... There is NO oov word.\")\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah... There is NO oov word.\n"
     ]
    }
   ],
   "source": [
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab.itos, w2v))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它 -- 也可以使embedding matrix一起更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确度评价指标\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        # 如果没指定device就使用net的device\n",
    "        device = list(net.parameters())[0].device\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval() # 评估模式, 这会关闭dropout\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            net.train() # 改回训练模式\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "\n",
    "def evaluate_auc(data_iter, net, device=None):\n",
    "    if device is None:\n",
    "        device = list(net.parameters())[0].device\n",
    "    y_true, y_hat = np.zeros(0), np.zeros(0)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval() # 评估模式, 这会关闭dropout\n",
    "            y_hat = np.concatenate([y_hat, softmax(net(X.to(device)).detach().cpu())[:,1].numpy()])\n",
    "            y_true = np.concatenate([y_true, y.cpu().numpy()])\n",
    "            net.train() # 改回训练模式\n",
    "    return roc_auc_score(y_true, y_hat), y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        train_auc, _ = evaluate_auc(train_iter, net)\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        test_auc, _ = evaluate_auc(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, train auc %.3f, test auc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, train_auc, test_auc, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 即尝试不同的超参的值\n",
    "\n",
    "简单说明几点：\n",
    "\n",
    "- 文本长度 max_len 也属于超参，之所以调整文本长度，是为了更好的并行\n",
    "- 一般假设训练数据和测试数据满足**独立同分布**，这里训练集和测试集都是短文本，最大长度为19\n",
    "- 按字embedding 比切词embedding要好一些，可能的数据量较小的原因\n",
    "- 也可能是数据量小的原因，模型迭代3次便开始过拟合\n",
    "- batch_size 一般是越大梯度下降的越精确，但是本任务中发现32的batch 略优于 64的batch\n",
    "- wiki预训练的词向量作为参数跟着一起train 略优于 fixed这些词向量\n",
    "- 用sigmoid输出数据（直接输出一个概率值，表示为1的概率） 分数略低于 softmax处理（分别输出0、1的概率）\n",
    "- 学习率的调整：先用0.001的学习率 train 3次，在用0.0002的学习率train2次\n",
    "- lstm层数\n",
    "- 尝试了Attention机制，但是由于当时时间比较紧迫，第一次的分数约在0.94+，就没深入调参。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.3619, train acc 0.836, test acc 0.890, train auc 0.953, test auc 0.955, time 21.9 sec\n",
      "epoch 2, loss 0.1299, train acc 0.892, test acc 0.912, train auc 0.964, test auc 0.966, time 21.3 sec\n",
      "epoch 3, loss 0.0763, train acc 0.909, test acc 0.920, train auc 0.972, test auc 0.972, time 21.3 sec\n"
     ]
    }
   ],
   "source": [
    "# 0.001的学习率先训练3次\n",
    "lr, num_epochs = 0.001, 3\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, valid_iter, net, loss, optimizer, device, num_epochs)  # 全部的数据参与训练，验证集还是最后2k的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.1881, train acc 0.926, test acc 0.928, train auc 0.978, test auc 0.979, time 21.0 sec\n",
      "epoch 2, loss 0.0876, train acc 0.932, test acc 0.940, train auc 0.981, test auc 0.982, time 21.0 sec\n"
     ]
    }
   ],
   "source": [
    "# 0.0002的学习率再训练2次\n",
    "net.train()\n",
    "lr, num_epochs = 0.0002, 2\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, valid_iter, net, loss, optimizer, device, num_epochs)  # 全部的数据参与训练，验证集还是最后2k的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "# 预测函数\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return label.item()\n",
    "\n",
    "# 输出结果\n",
    "with open(\"./data/Comments/test_handout.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "test_X, _ = preprocess_comments(lines, vocab, train_data=False)\n",
    "\n",
    "\n",
    "# test_y = torch.nn.Sigmoid()(net(test_X.to(device))[:, 1]).detach().cpu().numpy()\n",
    "test_y = softmax(net(test_X.to(device))).detach().cpu()[:,1].numpy()\n",
    "pd.DataFrame({\"ID\":range(0,len(test_y)),\"Prediction\":test_y}).to_csv(\"./data/Comments/final_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
